{"cells":[{"cell_type":"markdown","source":["Metadata (measurement files) for the datastreams are stored in a Cosmos DB. There is a set of REST api to simplify the access and discovery of the metadata. Azure key vault is used for storing any secrets/keys.\n\nThe following code will demonstrate how to make a REST call to query the measurement files.\n\nReferences -\n1. Secret management - https://learn.microsoft.com/en-us/azure/databricks/security/secrets/\n2. Databricks secrets using azure Key vault - https://hevodata.com/learn/databricks-secret/#51 \n\nReference to install the SSL certificates:\nhttps://learn.microsoft.com/en-us/azure/databricks/kb/python/import-custom-ca-cert"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"cdb8403b-4adc-4afa-8408-b77e43d48ec8","inputWidgets":{},"title":"Metadata discovery API"}}},{"cell_type":"code","source":["# Constants\nAPI_ENDPOINT = \"https://web-app-linuxrqpzw.azurewebsites.net\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0896963d-3902-4ce4-a1e6-32e15fb9d4cc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import requests\n# Example - https://web-app-linuxrqpzw.azurewebsites.net/v0.0.2/datastreams?measurementId=7e5dd17c-d299-41b9-8bb0-cac28b4c2b72&type=EXTRACTED&status=CREATED&tags=tag1%2Ctag2\ndef GetDatastreamsDataUriByMeasurement(measurementId, searchQuery):\n    api_url = f\"{API_ENDPOINT}/v1/datastreams?measurementId={measurementId}&{searchQuery}\"\n    response = requests.get(api_url, verify = False)\n    if response.json()['size'] < 1: return\n    # Replace https with abfss Format the Data Url from the response\n    return \"{0}/{1}\".format(response.json()['items'][0]['baseUriPath'].replace(\"https://\", \"abfss://\"), response.json()['items'][0]['relativeUriPath'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0882bdf9-1882-441b-8661-bb4058526cff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import requests\nrequests.packages.urllib3.disable_warnings() \n# Example - https://web-app-linuxrqpzw.azurewebsites.net/v1/datastreams?type=EXTRACTED&status=CREATED&tags=tag1\n# Returns Datastream Id and Datastream Data URI\ndef GetDatastreamsDataUri(searchQuery):\n    api_url = f\"{API_ENDPOINT}/v1/datastreams?{searchQuery}\"\n    response = requests.get(api_url, verify = False)\n    if response.json()['size'] < 1: return\n    # Replace https with abfss Format the Data Url from the response\n    return response.json()['items'][0]['id'],response.json()['items'][0]['measurementId'],\"{0}{1}@{2}/{3}\".format(\"abfss://\",response.json()['items'][0]['relativeUriPath'].split(\"/\",1)[0],response.json()['items'][0]['baseUriPath'].replace(\"https://\", \"\").replace(\"blob\",\"dfs\"),response.json()['items'][0]['relativeUriPath'].split(\"/\",1)[1])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7d53ce1-c5b8-46cc-a997-b1a3c7bbfed3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import requests\nrequests.packages.urllib3.disable_warnings()\n\ndef CreateDatastream(measurementId, data):\n    api_endpoint = f\"{API_ENDPOINT}/v1/measurements/{measurementId}/datastreams\"\n    headers = {\"Content-Type\": \"application/json\"}\n    response = requests.post(url = api_endpoint, headers= headers, data = data, verify = False)\n    curatedDatastreamDataUri = \"{0}{1}@{2}/{3}\".format(\"abfss://\",response.json()['relativeUriPath'].split(\"/\",1)[0],response.json()['baseUriPath'].replace(\"https://\", \"\").replace(\"blob\",\"dfs\"),response.json()['relativeUriPath'].split(\"/\",1)[1])\n    return curatedDatastreamDataUri"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2db2222f-b9a4-4bb0-98a0-fd70d7ab63bb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The datastreams are stored in a datalake(ADLS). Once the datastream url has been retrieved from the measurement storage(Cosmos DB), it can be used to access the datastream. There are multiple ways to authenticate to ADLS from databricks. A recommended way is to via OAuth with an Azure service principal.\n\nThe following code demonstrate how to authenticate and read a datastream from the datalake.\n\nReferences -\n1. Access Azure Data Lake Storage Gen2 or Blob Storage using OAuth 2.0 with an Azure service principal - https://learn.microsoft.com/en-us/azure/databricks/external-data/azure-storage#--access-azure-data-lake-storage-gen2-or-blob-storage-using-oauth-20-with-an-azure-service-principal"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"e22820ae-c327-4f20-b47c-093798ac206c","inputWidgets":{},"title":"Access datastream data"}}},{"cell_type":"code","source":["def set_spark_conf(storage_account_name,service_principal_id,service_principal_secret,tenant_id): \n    spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n    spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", service_principal_id)\n    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", service_principal_secret)\n    spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"688fc8e5-dc85-4581-b264-7554db8dc3d3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def ReadDatastream(datastreamDataUri, seperator, file_format):\n    df = spark.read.option(\"sep\", seperator).csv(f\"{datastreamDataUri}/*.{file_format}\")\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e6312d71-f3cb-419e-8808-db44cd49f63f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The curated data can be presented as a table in databricks. These tables external tables, so the actual data still sits in ADLS datalake. But they give a sql friendly semantic layer for working with it easily."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"1b5eb8dd-bb38-42ae-b5b3-c487f3d86351","inputWidgets":{},"title":"Curated Table"}}},{"cell_type":"code","source":["def write_table(dataframe, table_name, mode, location):\n    dataframe.write.\\\n        mode(mode).\\\n        format(\"parquet\").\\\n        option(\"path\", location).\\\n        saveAsTable(table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ae8edbe2-fb0e-4991-a8fb-b2565d92e1b2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"AV_Data_Discovery_Helper","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3195014550263348,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":39861663880844}},"nbformat":4,"nbformat_minor":0}
